{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFa3Qoj9NJ3l",
        "outputId": "9d827652-e01b-4a1f-ccf0-3c1971899d6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Load data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "push_normalized = pd.read_csv('/content/drive/MyDrive/MelodyResearch/final_csv/final_normalized/'+ 'push_normalized.csv')\n",
        "pull_normalized = pd.read_csv('/content/drive/MyDrive/MelodyResearch/final_csv/final_normalized/'+  'pull_normalized.csv')\n",
        "folding_normalized = pd.read_csv('/content/drive/MyDrive/MelodyResearch/final_csv/final_normalized/'+  'folding_normalized.csv')\n",
        "sponge_push_normalized = pd.read_csv('/content/drive/MyDrive/MelodyResearch/final_csv/final_normalized/'+  'sponge_push_normalized.csv')\n",
        "\n",
        "push_normalized_25 = pd.read_csv('/content/drive/MyDrive/MelodyResearch/final_csv/final_normalized/'+  'push_normalized_light_0.25.csv')\n",
        "push_normalized_50 = pd.read_csv('/content/drive/MyDrive/MelodyResearch/final_csv/final_normalized/'+  'push_normalized_medium_0.5.csv')\n",
        "push_normalized_75 = pd.read_csv('/content/drive/MyDrive/MelodyResearch/final_csv/final_normalized/'+  'push_normalized_heavy_0.75.csv')\n",
        "\n",
        "pull_normalized_25 = pd.read_csv('/content/drive/MyDrive/MelodyResearch/final_csv/final_normalized/'+  'pull_normalized_light_0.25.csv')\n",
        "pull_normalized_50 = pd.read_csv('/content/drive/MyDrive/MelodyResearch/final_csv/final_normalized/'+  'pull_normalized_medium_0.5.csv')\n",
        "pull_normalized_75 = pd.read_csv('/content/drive/MyDrive/MelodyResearch/final_csv/final_normalized/'+  'pull_normalized_heavy_0.75.csv')\n",
        "\n",
        "folding_normalized_25 = pd.read_csv('/content/drive/MyDrive/MelodyResearch/final_csv/final_normalized/'+  'folding_normalized_light_0.25.csv')\n",
        "folding_normalized_50 = pd.read_csv('/content/drive/MyDrive/MelodyResearch/final_csv/final_normalized/'+  'folding_normalized_medium_0.5.csv')\n",
        "folding_normalized_75 = pd.read_csv('/content/drive/MyDrive/MelodyResearch/final_csv/final_normalized/'+  'folding_normalized_heavy_0.75.csv')\n",
        "\n",
        "sponge_push_normalized_25 = pd.read_csv('/content/drive/MyDrive/MelodyResearch/final_csv/final_normalized/'+  'sponge_push_normalized_light_0.25.csv')\n",
        "sponge_push_normalized_50 = pd.read_csv('/content/drive/MyDrive/MelodyResearch/final_csv/final_normalized/'+  'sponge_push_normalized_medium_0.5.csv')\n",
        "sponge_push_normalized_75 = pd.read_csv('/content/drive/MyDrive/MelodyResearch/final_csv/final_normalized/'+  'sponge_push_normalized_heavy_0.75.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feature_columns = ['field.base.imu_acceleration_x',\n",
        "                     'field.base.imu_acceleration_y',\n",
        "                     'field.base.imu_acceleration_z',\n",
        "                     'field.base.imu_angular_velocity_x',\n",
        "                     'field.base.imu_angular_velocity_y',\n",
        "                     'field.base.imu_angular_velocity_z',\n",
        "                     'field.base.tool_pose_x',\n",
        "                     'field.base.tool_pose_y',\n",
        "                     'field.base.tool_pose_z',\n",
        "                     'field.base.tool_pose_theta_x',\n",
        "                     'field.base.tool_pose_theta_y',\n",
        "                     'field.base.tool_pose_theta_z',\n",
        "                     'field.base.tool_twist_linear_x',\n",
        "                     'field.base.tool_twist_linear_y',\n",
        "                     'field.base.tool_twist_linear_z',\n",
        "                     'field.base.tool_twist_angular_x',\n",
        "                     'field.base.tool_twist_angular_y',\n",
        "                     'field.base.tool_twist_angular_z',\n",
        "                     'field.base.commanded_tool_pose_x',\n",
        "                     'field.base.commanded_tool_pose_y',\n",
        "                     'field.base.commanded_tool_pose_z',\n",
        "                     'field.base.commanded_tool_pose_theta_x',\n",
        "                     'field.base.commanded_tool_pose_theta_y',\n",
        "                     'field.base.commanded_tool_pose_theta_z',\n",
        "                     'field.interconnect.imu_acceleration_x',\n",
        "                     'field.interconnect.imu_acceleration_y',\n",
        "                     'field.interconnect.imu_acceleration_z',\n",
        "                     'field.interconnect.imu_angular_velocity_x',\n",
        "                     'field.interconnect.imu_angular_velocity_y',\n",
        "                     'field.interconnect.imu_angular_velocity_z',\n",
        "                     'field.interconnect.oneof_tool_feedback.gripper_feedback0.motor0.position',\n",
        "                     'field.interconnect.oneof_tool_feedback.gripper_feedback0.motor0.velocity',\n",
        "                     'field.actuators0.position',\n",
        "                     'field.actuators1.position',\n",
        "                     'field.actuators2.position',\n",
        "                     'field.actuators3.position',\n",
        "                     'field.actuators4.position',\n",
        "                     'field.actuators5.position',\n",
        "                     'field.actuators0.velocity',\n",
        "                     'field.actuators1.velocity',\n",
        "                     'field.actuators2.velocity',\n",
        "                     'field.actuators3.velocity',\n",
        "                     'field.actuators4.velocity',\n",
        "                     'field.actuators5.velocity',\n",
        "                     'field.base.tool_external_wrench_force_x',\n",
        "                     'field.base.tool_external_wrench_force_y',\n",
        "                     'field.base.tool_external_wrench_force_z',\n",
        "                     'field.base.tool_external_wrench_torque_x',\n",
        "                     'field.base.tool_external_wrench_torque_y',\n",
        "                     'field.base.tool_external_wrench_torque_z',\n",
        "                     'field.actuators0.torque',\n",
        "                     'field.actuators1.torque',\n",
        "                     'field.actuators2.torque',\n",
        "                     'field.actuators3.torque',\n",
        "                     'field.actuators4.torque',\n",
        "                     'field.actuators5.torque']"
      ],
      "metadata": {
        "id": "ujR_KRZOPrjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "good_datasets = [push_normalized, pull_normalized, folding_normalized, sponge_push_normalized]\n",
        "dataset_names = [\"push_normalized\", \"pull_normalized\", \"folding_normalized\", \"sponge_push_normalized\"]\n",
        "\n"
      ],
      "metadata": {
        "id": "OrNeJwubnfqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "\n",
        "# Assuming you have the good datasets loaded into the 'good_datasets' list\n",
        "# Assuming you have the feature_columns list defined\n",
        "# Assuming you have the dataset_names list defined\n",
        "\n",
        "# Set plot parameters\n",
        "fig_width = 12  # Width of each subplot in inches\n",
        "fig_height = 5  # Height of each subplot in inches (increased from 4)\n",
        "num_plots_per_row = 4  # Number of plots to display in a row\n",
        "pdf_filename = 'plotGridColor.pdf'  # Name of the output PDF file\n",
        "\n",
        "# Define a list of colors for each dataset\n",
        "colors = ['blue', 'green', 'red', 'purple']\n",
        "\n",
        "# Create a PDF file to save the plots\n",
        "pdf_pages = PdfPages(pdf_filename)\n",
        "\n",
        "# Iterate over each feature\n",
        "for feature in feature_columns:\n",
        "    plt.figure(figsize=(fig_width, fig_height))\n",
        "\n",
        "    # Iterate over each dataset and plot the current feature\n",
        "    for dataset_idx, dataset in enumerate(good_datasets):\n",
        "        df = pd.DataFrame(dataset, columns=feature_columns)\n",
        "        plt.subplot(1, num_plots_per_row, dataset_idx + 1)\n",
        "\n",
        "        # Get the color for the current dataset\n",
        "        color = colors[dataset_idx % len(colors)]\n",
        "\n",
        "        plt.plot(df[feature], color=color)\n",
        "        plt.title(dataset_names[dataset_idx])\n",
        "        plt.xlabel('Time')\n",
        "        plt.ylabel('Value')\n",
        "\n",
        "    # Add the feature name as a title for the row\n",
        "    plt.suptitle(feature, y=0.95)  # Adjusted the y-coordinate for better visibility\n",
        "\n",
        "    # Adjust spacing between subplots and title\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.98])\n",
        "\n",
        "    # Save the plot to the PDF file\n",
        "    pdf_pages.savefig()\n",
        "    plt.close()\n",
        "\n",
        "# Close the PDF file\n",
        "pdf_pages.close()"
      ],
      "metadata": {
        "id": "JyJzDX7enJKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "# from tensorflow import keras\n",
        "# from tensorflow.keras import layers\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# # Concatenate all the good datasets into a single dataset\n",
        "# good_datasets = [push_normalized, pull_normalized, folding_normalized, sponge_push_normalized]\n",
        "# combined_good_data = np.concatenate(good_datasets)\n",
        "# print(combined_good_data)\n",
        "\n",
        "# # Concatenate all the bad datasets into a single dataset\n",
        "# bad_datasets = [push_normalized_25, push_normalized_50, push_normalized_75,\n",
        "#                 pull_normalized_25, pull_normalized_50, pull_normalized_75,\n",
        "#                 folding_normalized_25, folding_normalized_50, folding_normalized_75,\n",
        "#                 sponge_push_normalized_25, sponge_push_normalized_50, sponge_push_normalized_75]\n",
        "# combined_bad_data = np.concatenate(bad_datasets)\n",
        "# print(combined_bad_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nIipaDJsS7OM",
        "outputId": "3752fe67-ea52-4b75-c56c-75d8f2261166"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.2232349787933775 0.4404103544303659 0.6368912549596493 ... 0 0 0]\n",
            " [0.2181350308014931 0.461298769791135 0.6336184326490156 ... 0 0 0]\n",
            " [0.2181350308014931 0.461298769791135 0.6336184326490156 ... 0 0 0]\n",
            " ...\n",
            " [0.6904587234096746 0.2936691877085974 0.7302986548361687 ... 0 0 0]\n",
            " [0.6904587234096746 0.2936691877085974 0.7302986548361687 ... 0 0 0]\n",
            " [0.7451843574105441 0.3037245057396915 0.5552093627409246 ... 0 0 0]]\n",
            "[[0.3513952636437825 0.2974872139379754 0.5970024112532059 ... 1 1 1]\n",
            " [0.092450463108473 0.6043706826525465 0.5195378338276457 ... 1 1 1]\n",
            " [0.1475033595946475 0.439818785603312 0.5002339522892114 ... 1 1 1]\n",
            " ...\n",
            " [0.0 0.0 0.6661823701495615 ... 1 1 1]\n",
            " [1.0 0.5267876150759389 1.0 ... 1 1 1]\n",
            " [0.5106544565881681 1.0 0.2195003574818064 ... 1 1 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# # Create the labels for the good data (0 indicates good)\n",
        "# good_labels = np.zeros(combined_good_data.shape[0])\n",
        "\n",
        "# # Create the labels for the bad data (1 indicates bad)\n",
        "# bad_labels = np.ones(combined_bad_data.shape[0])\n",
        "\n",
        "# # Combine the good and bad datasets\n",
        "# combined_data = np.concatenate([combined_good_data, combined_bad_data])\n",
        "# combined_labels = np.concatenate([good_labels, bad_labels])\n",
        "\n",
        "# # Select the desired feature columns from your dataset\n",
        "# selected_data = combined_data[:, [feature_columns.index(col) for col in feature_columns]]\n",
        "\n",
        "# # Perform feature scaling if needed\n",
        "# scaler = StandardScaler()\n",
        "# scaled_data = scaler.fit_transform(selected_data)\n",
        "\n",
        "# # Split the data into training and testing sets\n",
        "# X_train, X_test, y_train, y_test = train_test_split(scaled_data, combined_labels, test_size=0.2, shuffle=False)\n",
        "\n",
        "# # Reshape the data for LSTM input (assuming the shape of your data is [samples, timesteps, features])\n",
        "# timesteps = 20  # Number of timesteps for each sequence\n",
        "# n_features = selected_data.shape[1]  # Number of features in each timestep\n",
        "\n",
        "# X_train = X_train.reshape(-1, timesteps, n_features)\n",
        "# X_test = X_test.reshape(-1, timesteps, n_features)\n",
        "\n",
        "# # Define the LSTM model\n",
        "# model = keras.Sequential()\n",
        "# model.add(layers.LSTM(64, input_shape=(timesteps, n_features)))\n",
        "# model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "# # Compile the model\n",
        "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# # Train the model\n",
        "# history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# # Evaluate the model on the testing set\n",
        "# loss, accuracy = model.evaluate(X_test, y_test)\n",
        "# print(f'Test Loss: {loss}, Test Accuracy: {accuracy}')\n",
        "\n",
        "# # Plot the training and validation accuracy over epochs\n",
        "# plt.plot(history.history['accuracy'])\n",
        "# plt.plot(history.history['val_accuracy'])\n",
        "# plt.title('Model Accuracy')\n",
        "# plt.xlabel('Epoch')\n",
        "# plt.ylabel('Accuracy')\n",
        "# plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "# plt.show()\n",
        "\n",
        "# # Plot the training and validation loss over epochs\n",
        "# plt.plot(history.history['loss'])\n",
        "# plt.plot(history.history['val_loss'])\n",
        "# plt.title('Model Loss')\n",
        "# plt.xlabel('Epoch')\n",
        "# plt.ylabel('Loss')\n",
        "# plt.legend(['Train', 'Validation'], loc='upper right')\n",
        "# plt.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "Qsp0NiCnNQKo",
        "outputId": "d4a1f499-c51e-4cdd-a14f-7076af9331e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-4db424649d71>\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselected_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Number of features in each timestep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimesteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimesteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 1788192 into shape (20,56)"
          ]
        }
      ]
    }
  ]
}